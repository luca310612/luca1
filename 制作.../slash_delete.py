import pandas as pd
import csv
import re
from pathlib import Path
import time

def clean_medical_data(csv_paths):
    """
    åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿CSVã‹ã‚‰ä¸è¦ãªæƒ…å ±ã‚’ä¸€æ‹¬å‰Šé™¤
    - IDå½¢å¼ã€Œæ•°å€¤,ã‚³ãƒ¼ãƒ‰ / æ–‡å­—åˆ—,æ•°å€¤,æ•°å€¤,ã€ã‚’ä¿æŒ
    - é›»è©±ç•ªå·ã‹ã‚‰ã€Œ/ å¸¸å‹¤:...ã€ä»¥é™ã‚’å‰Šé™¤
    - å„åˆ—ã‹ã‚‰åŒ»å¸«æ•°ãªã©ã®æƒ…å ±ã‚’å‰Šé™¤
    - å¤§é‡ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œã—ãŸé«˜é€Ÿå‡¦ç†
    """
    
    # å‰Šé™¤å¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ­£è¦è¡¨ç¾ï¼‰
    patterns_to_remove = [
        r'/ å¸¸ã€€å‹¤:.*',      # å¸¸å‹¤æƒ…å ±ä»¥é™ã‚’å‰Šé™¤
        r'/ éå¸¸å‹¤:.*',      # éå¸¸å‹¤æƒ…å ±ä»¥é™ã‚’å‰Šé™¤
        r'/ \(åŒ».*?\)',      # åŒ»å¸«æ•°æƒ…å ±ã‚’å‰Šé™¤
        r'/ \(æ­¯.*?\)',      # æ­¯ç§‘åŒ»å¸«æ•°æƒ…å ±ã‚’å‰Šé™¤
        r'/ \(è–¬.*?\)',      # è–¬å‰¤å¸«æ•°æƒ…å ±ã‚’å‰Šé™¤
        r'/ æ–°è¦.*',         # æ–°è¦æƒ…å ±ä»¥é™ã‚’å‰Šé™¤
        r'/ ä»¤\d+\..*',      # ä»¤å’Œå¹´æœˆæ—¥ä»¥é™ã‚’å‰Šé™¤
        r'/ å¹³\d+\..*',      # å¹³æˆå¹´æœˆæ—¥ä»¥é™ã‚’å‰Šé™¤
        r'/ æ˜­\d+\..*',      # æ˜­å’Œå¹´æœˆæ—¥ä»¥é™ã‚’å‰Šé™¤
        r'/ ç¾å­˜.*',         # ç¾å­˜æƒ…å ±ä»¥é™ã‚’å‰Šé™¤
    ]
    
    def preserve_id_format(text):
        """IDå½¢å¼ã€Œæ•°å€¤,ã‚³ãƒ¼ãƒ‰ / æ–‡å­—åˆ—,æ•°å€¤,æ•°å€¤,ã€ã‚’ä¿æŒ"""
        if pd.isna(text) or text == '':
            return text
        
        text = str(text)
        
        # IDå½¢å¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        # ä¾‹: "4.0,01 / å±±åŒ»17,1017,3,"
        id_pattern = r'^(\d+(?:\.\d+)?,\d+\s*/\s*[^,]+,\d+,\d+,?)(.*)$'
        match = re.match(id_pattern, text)
        
        if match:
            # IDéƒ¨åˆ†ã¨æ®‹ã‚Šã®éƒ¨åˆ†ã‚’åˆ†é›¢
            id_part = match.group(1)
            remaining_part = match.group(2)
            
            # æ®‹ã‚Šã®éƒ¨åˆ†ã‹ã‚‰ä¸è¦ãªæƒ…å ±ã‚’å‰Šé™¤
            cleaned_remaining = clean_text(remaining_part)
            
            # IDéƒ¨åˆ†ã‚’ä¿æŒã—ã¦çµåˆ
            if cleaned_remaining.strip():
                return f"{id_part}{cleaned_remaining}"
            else:
                return id_part
        else:
            # IDå½¢å¼ã§ãªã„å ´åˆã¯é€šå¸¸ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            return clean_text(text)
    
    def clean_text(text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¸è¦ãªæƒ…å ±ã‚’å‰Šé™¤"""
        if pd.isna(text) or text == '':
            return text
        
        # æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†
        text = str(text)
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é †æ¬¡å‰Šé™¤
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # æœ€å¾Œã®ã€Œ/ã€ãŒã‚ã‚Œã°å‰Šé™¤ã—ã¦ãƒˆãƒªãƒ 
        text = re.sub(r'/\s*$', '', text)
        return text.strip()
    
    def clean_tel_with_comma(text):
        """é›»è©±ç•ªå·å°‚ç”¨ï¼šã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã«ã‚«ãƒ³ãƒã‚’è¿½åŠ """
        if pd.isna(text) or text == '':
            return text
        
        # ã¾ãšé€šå¸¸ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned = clean_text(text)
        
        # é›»è©±ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹å ´åˆã®ã¿ã‚«ãƒ³ãƒã‚’è¿½åŠ 
        # æ—¥æœ¬ã®é›»è©±ç•ªå·å½¢å¼: 0XX-XXX-XXXX, 0XXX-XX-XXXX ãªã©
        tel_pattern = r'^\d{2,4}-\d{2,4}-\d{4}$'
        if re.match(tel_pattern, cleaned):
            return cleaned + ','
        
        # æ—¢ã«ã‚«ãƒ³ãƒãŒä»˜ã„ã¦ã„ã‚‹å ´åˆã¯ä½•ã‚‚ã—ãªã„
        if cleaned.endswith(','):
            return cleaned
        
        # é›»è©±ç•ªå·ã‚‰ã—ã„æ–‡å­—åˆ—ã®å ´åˆã‚‚ã‚«ãƒ³ãƒã‚’è¿½åŠ 
        # (æ•°å­—ã¨ãƒã‚¤ãƒ•ãƒ³ã®ã¿ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹å ´åˆ)
        if re.match(r'^[\d\-]+$', cleaned) and '-' in cleaned:
            return cleaned + ','
        
        return cleaned
    
    def is_likely_id_column(column_name, sample_values):
        """åˆ—ãŒIDå½¢å¼ã‚’å«ã‚€å¯èƒ½æ€§ãŒé«˜ã„ã‹ã‚’åˆ¤å®š"""
        if not column_name or len(sample_values) == 0:
            return False
        
        # åˆ—åãŒIDã£ã½ã„
        if column_name.lower() in ['id', 'code', 'name']:
            return True
        
        # ã‚µãƒ³ãƒ—ãƒ«å€¤ã‚’ãƒã‚§ãƒƒã‚¯
        id_pattern = r'\d+(?:\.\d+)?,\d+\s*/\s*[^,]+,\d+,\d+'
        for value in sample_values[:5]:  # æœ€åˆã®5ã¤ã‚’ãƒã‚§ãƒƒã‚¯
            if pd.notna(value) and re.search(id_pattern, str(value)):
                return True
        
        return False
    
    # å‡¦ç†å¯¾è±¡ã®ä¸»è¦åˆ—ï¼ˆã“ã‚Œã‚‰ã®åˆ—ã‚’é‡ç‚¹çš„ã«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
    target_columns = [
        'tel', 'name', 'address', 'corporation', 'director', 
        'established', 'bed_type_and_count', 'facility_type'
    ]
    
    total_files = len(csv_paths)
    
    for file_idx, csv_path in enumerate(csv_paths, 1):
        print(f"\n{'='*60}")
        print(f"å‡¦ç†ä¸­ ({file_idx}/{total_files}): {csv_path}")
        print(f"{'='*60}")
        
        start_time = time.time()
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
            if not Path(csv_path).exists():
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
                continue
                
            # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
            df = None
            read_methods = [
                # æ–¹æ³•1: æ¨™æº–çš„ãªèª­ã¿è¾¼ã¿
                lambda: pd.read_csv(csv_path, dtype=str, encoding='utf-8-sig'),
                # æ–¹æ³•2: å•é¡Œè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                lambda: pd.read_csv(csv_path, dtype=str, encoding='utf-8-sig', on_bad_lines='skip'),
                # æ–¹æ³•3: Pythonã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
                lambda: pd.read_csv(csv_path, dtype=str, encoding='utf-8-sig', engine='python', on_bad_lines='skip'),
                # æ–¹æ³•4: åŒºåˆ‡ã‚Šæ–‡å­—ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
                lambda: pd.read_csv(csv_path, dtype=str, encoding='utf-8-sig', sep=',', engine='python', on_bad_lines='skip'),
            ]
            
            for method_idx, read_method in enumerate(read_methods, 1):
                try:
                    df = read_method()
                    print(f"âœ… èª­ã¿è¾¼ã¿æˆåŠŸ (æ–¹æ³•{method_idx}): {len(df):,}è¡Œ, {len(df.columns)}åˆ—")
                    break
                except Exception as e:
                    print(f"âŒ èª­ã¿è¾¼ã¿æ–¹æ³•{method_idx}å¤±æ•—: {str(e)[:100]}...")
                    continue
            
            if df is None:
                print(f"âŒ å…¨ã¦ã®èª­ã¿è¾¼ã¿æ–¹æ³•ãŒå¤±æ•—ã—ã¾ã—ãŸ: {csv_path}")
                continue
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
            print(f"ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
            
            # å„åˆ—ã®å‡¦ç†å‰ä¾‹ã‚’è¡¨ç¤º
            print(f"\nğŸ“‹ å‡¦ç†å‰ã®ä¾‹:")
            for col in df.columns[:3]:  # æœ€åˆã®3åˆ—ã‚’è¡¨ç¤º
                if len(df) > 0 and pd.notna(df[col].iloc[0]):
                    example = str(df[col].iloc[0])
                    print(f"   {col}: {example[:100]}...")
            
            # å…¨åˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            cleaned_count = 0
            for col in df.columns:
                original_values = df[col].copy()
                
                # åˆ—ã®æ€§è³ªã‚’åˆ¤å®š
                sample_values = df[col].dropna().head(10).tolist()
                is_id_col = is_likely_id_column(col, sample_values)
                
                if col == 'tel':
                    # é›»è©±ç•ªå·åˆ—ã¯å°‚ç”¨é–¢æ•°ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆã‚«ãƒ³ãƒä»˜ãï¼‰
                    df[col] = df[col].apply(clean_tel_with_comma)
                elif is_id_col:
                    # IDå½¢å¼ã‚’å«ã‚€åˆ—ã¯å°‚ç”¨é–¢æ•°ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    df[col] = df[col].apply(preserve_id_format)
                    print(f"   ğŸ”¢ {col}: IDå½¢å¼ä¿æŒãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†")
                elif col in target_columns:
                    # é‡è¦ãªåˆ—ã¯ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    df[col] = df[col].apply(clean_text)
                else:
                    # ãã®ä»–ã®åˆ—ã‚‚è»½ãã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    df[col] = df[col].apply(lambda x: clean_text(x) if isinstance(x, str) and ('å¸¸ã€€å‹¤:' in x or 'éå¸¸å‹¤:' in x) else x)
                
                # å¤‰æ›´ã•ã‚ŒãŸè¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                changed = (original_values != df[col]).sum()
                if changed > 0:
                    cleaned_count += changed
                    print(f"   âœ… {col}: {changed:,}è¡Œã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°")
            
            # å‡¦ç†å¾Œã®ä¾‹ã‚’è¡¨ç¤º
            print(f"\nğŸ“‹ å‡¦ç†å¾Œã®ä¾‹:")
            for col in df.columns[:3]:  # æœ€åˆã®3åˆ—ã‚’è¡¨ç¤º
                if len(df) > 0 and pd.notna(df[col].iloc[0]):
                    example = str(df[col].iloc[0])
                    print(f"   {col}: {example}")
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_path = csv_path + '.backup'
            if not Path(backup_path).exists():
                Path(csv_path).rename(backup_path)
                print(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            df.to_csv(csv_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL)
            
            # å‡¦ç†æ™‚é–“è¨ˆç®—
            elapsed_time = time.time() - start_time
            
            print(f"\nâœ… å‡¦ç†å®Œäº†!")
            print(f"   ğŸ“Š ç·ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ•°: {cleaned_count:,}ç®‡æ‰€")
            print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
            print(f"   ğŸ’¾ ä¿å­˜å®Œäº†: {csv_path}")
            
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {csv_path}")
            print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
            import traceback
            traceback.print_exc()
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ å…¨ä½“å‡¦ç†å®Œäº†! ({total_files}ãƒ•ã‚¡ã‚¤ãƒ«)")
    print(f"{'='*60}")

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
    csv_file_paths = [
        'csv/yamagata_hos.csv',
        # ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã“ã“ã«è¿½åŠ 
        # 'csv/other_file1.csv',
        # 'csv/other_file2.csv',
    ]
    
    print("ğŸš€ å¤§é‡ãƒ‡ãƒ¼ã‚¿ CSV ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ï¼ˆIDå½¢å¼ä¿æŒç‰ˆï¼‰")
    print(f"ğŸ“ å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(csv_file_paths)}")
    
    clean_medical_data(csv_file_paths)